{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import librosa\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "from langdetect import detect\n",
    "import pickle\n",
    "import hashlib\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import logging\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "import soundfile as sf\n",
    "import threading\n",
    "import traceback\n",
    "\n",
    "# Descargar recursos NLTK necesarios\n",
    "def download_nltk_resources():\n",
    "    resources = ['punkt', 'stopwords', 'wordnet', 'omw-1.4']\n",
    "    for resource in resources:\n",
    "        nltk.download(resource, quiet=True)\n",
    "download_nltk_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.DESKTOP_PATH = os.path.expanduser(\"~/Desktop\")\n",
    "        self.JARVIS_DATA_PATH = os.path.join(self.DESKTOP_PATH, \"jarvis_data\")\n",
    "        self.DATA_BD_PATH = os.path.join(self.JARVIS_DATA_PATH, \"databd\")\n",
    "        self.PREPROCESSED_PATH = os.path.join(self.JARVIS_DATA_PATH, \"preprocessed\")\n",
    "        self.CACHE_PATH = os.path.join(self.JARVIS_DATA_PATH, \"cache\")\n",
    "        self.CURRENT_DATE = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        self.CURRENT_PREPROCESSED_PATH = os.path.join(self.PREPROCESSED_PATH, self.CURRENT_DATE)\n",
    "        self.create_directories()\n",
    "        self.setup_logging()\n",
    "\n",
    "    def create_directories(self):\n",
    "        directories = [\n",
    "            self.JARVIS_DATA_PATH,\n",
    "            self.DATA_BD_PATH,\n",
    "            self.PREPROCESSED_PATH,\n",
    "            self.CURRENT_PREPROCESSED_PATH,\n",
    "            self.CACHE_PATH\n",
    "        ]\n",
    "        for directory in directories:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    def setup_logging(self):\n",
    "        logs_dir = os.path.join(self.JARVIS_DATA_PATH, \"logs\")\n",
    "        os.makedirs(logs_dir, exist_ok=True)\n",
    "        log_file = os.path.join(logs_dir, f'jarvis_preprocessing_{self.CURRENT_DATE}.log')\n",
    "        logging.basicConfig(\n",
    "            level=logging.DEBUG,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_file, encoding='utf-8'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CacheManager:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.cache_dir = config.CACHE_PATH\n",
    "        self.cache = {}\n",
    "        self.load_cache()\n",
    "\n",
    "    def get_hash(self, data):\n",
    "        return hashlib.md5(str(data).encode()).hexdigest()\n",
    "\n",
    "    def load_cache(self):\n",
    "        cache_file = os.path.join(self.cache_dir, 'preprocessing_cache.pkl')\n",
    "        if os.path.exists(cache_file):\n",
    "            with open(cache_file, 'rb') as f:\n",
    "                self.cache = pickle.load(f)\n",
    "\n",
    "    def save_cache(self):\n",
    "        cache_file = os.path.join(self.cache_dir, 'preprocessing_cache.pkl')\n",
    "        with open(cache_file, 'wb') as f:\n",
    "            pickle.dump(self.cache, f)\n",
    "\n",
    "    def get_cached(self, data):\n",
    "        data_hash = self.get_hash(data)\n",
    "        return self.cache.get(data_hash)\n",
    "\n",
    "    def add_to_cache(self, data, processed_data):\n",
    "        data_hash = self.get_hash(data)\n",
    "        self.cache[data_hash] = processed_data\n",
    "        self.save_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionalFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.nlp = spacy.load('es_core_news_lg')\n",
    "        except OSError:\n",
    "            os.system('python -m spacy download es_core_news_lg')\n",
    "            self.nlp = spacy.load('es_core_news_lg')\n",
    "        self.context_memory = {}\n",
    "\n",
    "    def extract_emotional_features(self, text):\n",
    "        features = {\"basic_analysis\": TextBlob(text).sentiment.polarity}\n",
    "        if self.nlp:\n",
    "            doc = self.nlp(text)\n",
    "            features.update({\n",
    "                \"entity_emotions\": self._analyze_entity_emotions(doc),\n",
    "                \"emotional_context\": self._analyze_emotional_context(text)\n",
    "            })\n",
    "        return features\n",
    "\n",
    "    def _analyze_entity_emotions(self, doc):\n",
    "        return {}\n",
    "\n",
    "    def _analyze_emotional_context(self, text):\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedDataPreprocessor:\n",
    "    def __init__(self, config=None):\n",
    "        self.config = config or {}\n",
    "        self.input_folders = self.config.get('data_paths', [])\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.processed_data = None\n",
    "        self.setup_tools()\n",
    "\n",
    "    def setup_tools(self):\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        self.stop_words = set(stopwords.words('spanish') + stopwords.words('english'))\n",
    "        try:\n",
    "            self.nlp = spacy.load('es_core_news_sm')\n",
    "        except OSError:\n",
    "            os.system('python -m spacy download es_core_news_sm')\n",
    "            self.nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [token for token in tokens if token not in self.stop_words]\n",
    "        doc = self.nlp(' '.join(tokens))\n",
    "        return ' '.join([token.lemma_ for token in doc])\n",
    "    \n",
    "    def process_files_parallel(self):\n",
    "        for input_folder in self.input_folders:\n",
    "            files = self._get_files_to_process(input_folder)\n",
    "            with ThreadPoolExecutor(max_workers=os.cpu_count() or 4) as executor:\n",
    "                results = list(executor.map(self._process_file, files))\n",
    "            return results\n",
    "\n",
    "    def _get_files_to_process(self, folder):\n",
    "        return [\n",
    "            os.path.join(root, file)\n",
    "            for root, _, files in os.walk(folder)\n",
    "            for file in files if file.endswith(('.txt', '.json'))\n",
    "        ]\n",
    "\n",
    "    def _process_file(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return self.preprocess_text(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    config_instance = Config()\n",
    "    preprocessor = AdvancedDataPreprocessor({'data_paths': [config_instance.DATA_BD_PATH]})\n",
    "    preprocessor.process_files_parallel()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
